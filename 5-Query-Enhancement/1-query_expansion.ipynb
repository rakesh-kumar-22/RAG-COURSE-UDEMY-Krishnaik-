{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08849184",
   "metadata": {},
   "source": [
    "### query enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd8e6af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbab212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc loader\n",
    "loader=TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs=loader.load()\n",
    "\n",
    "# text splitting\n",
    "\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "chunks=splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3ceda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore=FAISS.from_documents(chunks,embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6feff065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever\n",
    "retriever=vectorstore.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db91091e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='de313744-9b84-4453-9c78-fb869657ac56', metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain is an open-source framework designed for developing applications powered by large language models (LLMs). It simplifies the process of building, managing, and scaling complex chains of thought by abstracting prompt management, retrieval, memory, and agent orchestration. Developers can use LangChain to create end-to-end pipelines that connect LLMs with tools, APIs, vector databases, and other knowledge sources. (v8)'),\n",
       " Document(id='3715d2e7-7d34-4843-85a2-10a86b660f9f', metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='At the heart of LangChain lies the concept of chains, which are sequences of calls to LLMs and other tools. Chains can be simple, such as a single prompt fed to an LLM, or complex, involving multiple conditionally executed steps. LangChain makes it easy to compose and reuse chains using standard patterns like Stuff, Map-Reduce, and Refine. (v1)'),\n",
       " Document(id='59f3709e-526a-4206-abe5-1cad9817cce2', metadata={'source': 'langchain_crewai_dataset.txt'}, page_content='LangChain is an open-source framework designed for developing applications powered by large language models (LLMs). It simplifies the process of building, managing, and scaling complex chains of thought by abstracting prompt management, retrieval, memory, and agent orchestration. Developers can use LangChain to create end-to-end pipelines that connect LLMs with tools, APIs, vector databases, and other knowledge sources. (v6)')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddfa2ce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001E73F15CE10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001E73F15E850>, root_client=<openai.OpenAI object at 0x000001E73F15E490>, root_async_client=<openai.AsyncOpenAI object at 0x000001E73F15E5D0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "llm=init_chat_model(\"openai:o4-mini\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0af3eadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY Enhancement\n",
    "query_expansion_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\n",
    "\n",
    "Original query: \"{query}\"\n",
    "\n",
    "Expanded query:\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af790917",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_expansion_chain=query_expansion_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bb1c1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Expanded query:\\n\\n“langchain memory” OR “LangChain Memory Module” OR “LangChain State Management” OR “conversation memory in LangChain” OR “LangChain context persistence” OR “chatbot session memory” OR “conversation history storage” OR “context window management” OR “memory retriever” OR “retrieval-augmented generation (RAG)” OR “vector-store memory” OR “embedding store” OR “persistent memory (Chroma, Pinecone, Redis)” OR “in-memory cache” OR “short-term vs. long-term memory retention” OR “LangChain Memory API” OR “contextual memory management”'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expansion_chain.invoke({\"query\":\"langchain memory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d2d06a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nanswer the question given only the context below \\ncontext:\\n{context}\\n\\nquestion:{input}\\n    ')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG answering prompt\n",
    "answer_prompt=PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "answer the question given only the context below \n",
    "context:\n",
    "{context}\n",
    "\n",
    "question:{input}\n",
    "    \"\"\"\n",
    ")\n",
    "answer_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd5a35ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  input: RunnableLambda(...),\n",
       "  context: RunnableLambda(...)\n",
       "}\n",
       "| RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "    context: RunnableLambda(format_docs)\n",
       "  }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "  | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nanswer the question given only the context below \\ncontext:\\n{context}\\n\\nquestion:{input}\\n    ')\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001E73F15CE10>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001E73F15E850>, root_client=<openai.OpenAI object at 0x000001E73F15E490>, root_async_client=<openai.AsyncOpenAI object at 0x000001E73F15E5D0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "  | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain=create_stuff_documents_chain(llm,prompt=answer_prompt)\n",
    "rag_chain=(\n",
    "    RunnableMap(\n",
    "        {\n",
    "            \"input\": lambda x:x['input'],\n",
    "            \"context\": lambda x:retriever.invoke( (query_expansion_chain.invoke({\"query\":x['input']})))\n",
    "        }\n",
    "    )\n",
    "    | document_chain\n",
    ")\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47d2b2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:  \n",
      "“What types of memory (also called memory modules, context-retention layers or stateful storage) does the LangChain Python SDK support for conversational agents and retrieval-augmented generation? For example:  \n",
      "- Short-term/ephemeral memory such as ConversationBufferMemory, TokenBufferMemory, Window (sliding-window) memory  \n",
      "- Long-term/persistent memory such as ConversationSummaryMemory, EntityMemory or KnowledgeGraphMemory  \n",
      "- Vector-store memory backends (e.g. FAISS, Pinecone, Weaviate), Redis/SQL/file-based memory stores  \n",
      "- Memory abstractions and interfaces (Memory class, memory retriever, callback manager)  \n",
      "Key synonyms and related terms: memory management, context window, session persistence, chat history storage, context caching, state management, memory adapters, memory plugins—within the broader LangChain framework for RAG, agents, and LLM-driven applications.”\n",
      "✅ Answer:\n",
      " LangChain currently provides memory modules such as:  \n",
      "• ConversationBufferMemory – keeps a running buffer of past turns in the dialogue.  \n",
      "• ConversationSummaryMemory – maintains and updates a concise summary of the interaction to stay within token limits.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"What types of memory does LangChain support?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_chain.invoke(query)\n",
    "print(\"✅ Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd8169f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here’s one possible expanded query that adds synonyms, related technical terms and useful context for better recall:\n",
      "\n",
      "(\"CrewAI agents\"  \n",
      " OR \"Crew.ai autonomous agents\"  \n",
      " OR \"AI‐driven crew management agents\"  \n",
      " OR \"intelligent crew scheduling agents\"  \n",
      " OR \"autonomous workforce planning agents\"  \n",
      " OR \"digital crew assistants\"  \n",
      " OR \"virtual crew planners\"  \n",
      " OR \"multi‐agent crewing system\")  \n",
      "AND  \n",
      "(\"crew management\"  \n",
      " OR \"crew scheduling\"  \n",
      " OR \"roster planning\"  \n",
      " OR \"shift allocation\"  \n",
      " OR \"resource optimization\"  \n",
      " OR \"workforce optimization\")  \n",
      "AND  \n",
      "(\"machine learning\"  \n",
      " OR \"reinforcement learning\"  \n",
      " OR \"multi‐agent systems\"  \n",
      " OR \"decision support systems\"  \n",
      " OR \"optimization algorithms\"  \n",
      " OR \"conversational AI\"  \n",
      " OR \"digital assistants\")\n",
      "\n",
      "✅ Answer:\n",
      " CrewAI agents are autonomous, LLM-powered “crew members” each defined by:  \n",
      "1. A clear purpose and goal  \n",
      "2. A role within the crew (e.g. researcher, planner, executor)  \n",
      "3. A set of tools or APIs they’re allowed to invoke  \n",
      "\n",
      "They’re orchestrated to share context, divide responsibilities, and dynamically communicate so that—working together—they accomplish the overall task.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"CrewAI agents?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_chain.invoke(query)\n",
    "print(\"\\n✅ Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304efda6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

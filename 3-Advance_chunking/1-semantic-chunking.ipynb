{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b19354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\Desktop\\RAGUdemy\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a48f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Semantic chunking\n",
      "\n",
      "\n",
      "chunk1:\n",
      " LangChain is a framework for building applications with LLMs. Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
      "\n",
      "chunk2:\n",
      " You can create chains, agents, memory, and retrievers.\n",
      "\n",
      "chunk3:\n",
      " The Eiffel Tower is located in Paris.\n",
      "\n",
      "chunk4:\n",
      " France is a popular tourist destination.\n"
     ]
    }
   ],
   "source": [
    "#Initialize the model\n",
    "model=SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "#sample text \n",
    "text=\"\"\"\n",
    "LangChain is a framework for building applications with LLMs.\n",
    "Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
    "You can create chains, agents, memory, and retrievers.\n",
    "The Eiffel Tower is located in Paris.\n",
    "France is a popular tourist destination.\n",
    "\"\"\"\n",
    "\n",
    "# step-1:split into sentences\n",
    "sentences=[i.strip() for i in text.split(\"\\n\") if i.strip() ]\n",
    "\n",
    "# step-2: embed each sentences\n",
    "embeddings=model.encode(sentences)\n",
    "\n",
    "# initialize threshold\n",
    "threshold=0.75\n",
    "chunks=[]\n",
    "current_chunk=[sentences[0]]\n",
    "\n",
    "# step-4: semantic grouping based on threshold\n",
    "for i in range(1,len(embeddings)):\n",
    "    sim=cosine_similarity(\n",
    "        [embeddings[i-1]],\n",
    "        [embeddings[i]]\n",
    "    )[0][0]\n",
    "\n",
    "    if sim>=threshold:\n",
    "        current_chunk.append(sentences[i])\n",
    "    else:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "        current_chunk=[sentences[i]]\n",
    "    \n",
    "# append the last chunk\n",
    "chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "# print the chunks\n",
    "print(\"\\n Semantic chunking\\n\")\n",
    "for idx,chunk in enumerate(chunks):\n",
    "    print(f\"\\nchunk{idx+1}:\\n {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3721ea4c",
   "metadata": {},
   "source": [
    "### RAG Pipeline Modular Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1192e8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableMap\n",
    "from langchain.prompts import PromptTemplate\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c636cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31aaa190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for semantic chunking\n",
    "class ThresholdSemanticChunker:\n",
    "    def __init__(self,model=\"all-MiniLM-L6-v2\",threshold=0.7):\n",
    "        self.model=SentenceTransformer(model)\n",
    "        self.threshold=threshold\n",
    "\n",
    "    def split(self,text:str):\n",
    "        sentences=[s.strip() for s in text.split(\"\\n\") if s.strip()]\n",
    "\n",
    "        # convert each sentence into vector\n",
    "        embeddings=self.model.encode(sentences)\n",
    "\n",
    "        chunks=[]\n",
    "        current_chunk=[sentences[0]]\n",
    "        for i in range(1,len(sentences)):\n",
    "            sim=cosine_similarity([embeddings[i-1]],[embeddings[i]])[0][0]\n",
    "\n",
    "            if sim>=self.threshold:\n",
    "                current_chunk.append(sentences[i])\n",
    "            else:\n",
    "                chunks.append(\".\".join(current_chunk)+\".\")\n",
    "                current_chunk=[sentences[i]]\n",
    "        chunks.append(\".\".join(current_chunk)+\".\")\n",
    "        return chunks\n",
    "    \n",
    "    def split_document(self,docs):\n",
    "        result=[]\n",
    "        for doc in docs:\n",
    "            for chunk in self.split(doc.page_content):\n",
    "                result.append(Document(page_content=chunk,metadata=doc.metadata))\n",
    "        return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d99024a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='\\nLangChain is a framework for building applications with LLMs.\\nLangchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\\nYou can create chains, agents, memory, and retrievers.\\nThe Eiffel Tower is located in Paris.\\nFrance is a popular tourist destination.\\n')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample text\n",
    "sample_text = \"\"\"\n",
    "LangChain is a framework for building applications with LLMs.\n",
    "Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
    "You can create chains, agents, memory, and retrievers.\n",
    "The Eiffel Tower is located in Paris.\n",
    "France is a popular tourist destination.\n",
    "\"\"\"\n",
    "\n",
    "doc=Document(page_content=sample_text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff260693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='LangChain is a framework for building applications with LLMs..Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone..'),\n",
       " Document(metadata={}, page_content='You can create chains, agents, memory, and retrievers..'),\n",
       " Document(metadata={}, page_content='The Eiffel Tower is located in Paris..'),\n",
       " Document(metadata={}, page_content='France is a popular tourist destination..')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chunking\n",
    "chunker=ThresholdSemanticChunker()\n",
    "chunks=chunker.split_document([doc])\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abba66d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore=FAISS.from_documents(chunks,embedding)\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df35a66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt template\n",
    "template=\"\"\"\n",
    "    answer the question based only on the following context:{context}\n",
    "    Question:{question}\n",
    "\n",
    "    \"\"\"\n",
    "prompt=PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c82885f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n    answer the question based only on the following context:{context}\\n    Question:{question}\\n\\n    ')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4bd577f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001940E522510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001940E522A50>, root_client=<openai.OpenAI object at 0x000001940E570E10>, root_async_client=<openai.AsyncOpenAI object at 0x000001940E5716D0>, temperature=0.4, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the llm\n",
    "llm=init_chat_model(\"openai:gpt-3.5-turbo\",temperature=0.4)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68a6c661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: RunnableLambda(...),\n",
       "  question: RunnableLambda(...)\n",
       "}\n",
       "| PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n    answer the question based only on the following context:{context}\\n    Question:{question}\\n\\n    ')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001940E522510>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001940E522A50>, root_client=<openai.OpenAI object at 0x000001940E570E10>, root_async_client=<openai.AsyncOpenAI object at 0x000001940E5716D0>, temperature=0.4, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LCEL Chain with retrieval\n",
    "rag_chain=(\n",
    "    RunnableMap(\n",
    "        {\n",
    "        \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "        \"question\": lambda x: x[\"question\"],  \n",
    "        }\n",
    "    )\n",
    "    |prompt\n",
    "    |llm\n",
    "    |StrOutputParser()\n",
    ")\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30b49d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is used as a framework for building applications with LLMs and provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n"
     ]
    }
   ],
   "source": [
    "query = {\"question\": \"What is LangChain used for?\"}\n",
    "result = rag_chain.invoke(query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020b8e90",
   "metadata": {},
   "source": [
    "### Semantic chunker With Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45d6866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9545d8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "chunk1:\n",
      "LangChain is a framework for building applications with LLMs. Langchain provides modular abstractions to combine LLMs with tools like OpenAI and Pinecone.\n",
      "\n",
      "chunk2:\n",
      "You can create chains, agents, memory, and retrievers. The Eiffel Tower is located in Paris. France is a popular tourist destination.\n"
     ]
    }
   ],
   "source": [
    "# load text\n",
    "loader=TextLoader(\"langchain_intro.txt\")\n",
    "docs=loader.load()\n",
    "\n",
    "# initialize embedding model\n",
    "embeddings=OpenAIEmbeddings()\n",
    "\n",
    "# Semantic chunker\n",
    "chunker=SemanticChunker(embeddings)\n",
    "\n",
    "# split the chunks\n",
    "chunks=chunker.split_documents(docs)\n",
    "\n",
    "for i,chunk in enumerate(chunks):\n",
    "    print(f\"\\nchunk{i+1}:\\n{chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc2811e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db=FAISS.from_documents(chunks,embeddings)\n",
    "retriever=db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44bb09bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n    answer the questions given context only\\n    {context}\\n    Question:{question}\\n    ')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template=\"\"\"\n",
    "    answer the questions given context only\n",
    "    {context}\n",
    "    Question:{question}\n",
    "    \"\"\"\n",
    "prompt=PromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "467879c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000019418672990>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000019418672350>, root_client=<openai.OpenAI object at 0x0000019419D05D90>, root_async_client=<openai.AsyncOpenAI object at 0x0000019419D05910>, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize llm\n",
    "llm=init_chat_model(\"openai:gpt-3.5-turbo\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d206f496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context: RunnableLambda(...),\n",
       "  question: RunnableLambda(...)\n",
       "}\n",
       "| PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\n    answer the questions given context only\\n    {context}\\n    Question:{question}\\n    ')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000019418672990>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000019418672350>, root_client=<openai.OpenAI object at 0x0000019419D05D90>, root_async_client=<openai.AsyncOpenAI object at 0x0000019419D05910>, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain2=(\n",
    "    RunnableMap(\n",
    "        {\n",
    "            \"context\":lambda x: retriever.invoke(x['question']),\n",
    "            \"question\": lambda x: x[\"question\"]\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b88a4d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is used for building applications with LLMs and combining them with tools like OpenAI and Pinecone.\n"
     ]
    }
   ],
   "source": [
    "query = {\"question\": \"What is LangChain used for?\"}\n",
    "result = rag_chain2.invoke(query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f2fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7603ab92",
   "metadata": {},
   "source": [
    "## Maximal Marignal Retriever(MMR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77c87fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "229bd679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# Document load\n",
    "loader=TextLoader(\"mmr-rag-dataset.txt\",encoding=\"utf-8\")\n",
    "docs=loader.load()\n",
    "print(len(docs))\n",
    "# Split documents\n",
    "splitter=RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "chunks=splitter.split_documents(docs)\n",
    "print(len(chunks))\n",
    "#Embeding model\n",
    "embedding_model=OpenAIEmbeddings()\n",
    "\n",
    "# vectorstore Faiss\n",
    "vectorstore=FAISS.from_documents(chunks,embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b0265a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000202D651F250>, search_type='mmr', search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MMR Retriever\n",
    "\n",
    "mmr_retriever=vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\":5}\n",
    ")\n",
    "mmr_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c70ed62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template=\"\\n    Answer the given uery using only the given context:{context}\\n    if you have not sufficient information then just say i don't know.\\n    question:{input} \\n        \")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "prompt=PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the given uery using only the given context:{context}\n",
    "    if you have not sufficient information then just say i don't know.\n",
    "    question:{input} \n",
    "        \"\"\"\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6ff8b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000202D66BA190>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000202D66BA3F0>, root_client=<openai.OpenAI object at 0x00000202D66CA5D0>, root_async_client=<openai.AsyncOpenAI object at 0x00000202D66CB020>, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize llm\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm=init_chat_model(\"openai:gpt-3.5-turbo\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f251a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create document chain and retrieval chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "doc_chain=create_stuff_documents_chain(llm,prompt)\n",
    "rag_chain=create_retrieval_chain(mmr_retriever,doc_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36173be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000202D651F250>, search_type='mmr', search_kwargs={'k': 5}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template=\"\\n    Answer the given uery using only the given context:{context}\\n    if you have not sufficient information then just say i don't know.\\n    question:{input} \\n        \")\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x00000202D66BA190>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000202D66BA3F0>, root_client=<openai.OpenAI object at 0x00000202D66CA5D0>, root_async_client=<openai.AsyncOpenAI object at 0x00000202D66CB020>, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "68e1ab2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How does LangChain support agents and memory?',\n",
       " 'context': [Document(id='b6fafd5e-5f6e-4077-8052-35b326e54b90', metadata={'source': 'mmr-rag-dataset.txt'}, page_content='LangChain supports conversational memory using ConversationBufferMemory and summarization memory with ConversationSummaryMemory.'),\n",
       "  Document(id='2c45a5ac-9477-463f-acd0-adaf0d1ffe9d', metadata={'source': 'mmr-rag-dataset.txt'}, page_content='Agents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.'),\n",
       "  Document(id='bd35d2d3-dc27-44ed-bc00-037288f9a3bc', metadata={'source': 'mmr-rag-dataset.txt'}, page_content='MMR (Maximal Marginal Relevance) retrieval in LangChain improves diversity by balancing relevance and redundancy.'),\n",
       "  Document(id='7ac85a28-b003-41b2-aeec-e8ede3f25451', metadata={'source': 'mmr-rag-dataset.txt'}, page_content='LangChain agents can interact with external APIs and databases, enhancing the capabilities of LLM-powered applications.'),\n",
       "  Document(id='9ac381b4-b361-44b4-b0cb-6ba2267b62ef', metadata={'source': 'mmr-rag-dataset.txt'}, page_content='Chroma is a lightweight vector store often used in LangChain for embedding-based document storage and retrieval.')],\n",
       " 'answer': 'LangChain supports agents with conversational memory using ConversationBufferMemory and summarization memory with ConversationSummaryMemory. Agents can also interact with external APIs and databases, use tools like calculators, search APIs, or custom functions, and utilize MMR retrieval for improved diversity. Additionally, Chroma is often used for embedding-based document storage and retrieval in LangChain.'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = {\"input\": \"How does LangChain support agents and memory?\"}\n",
    "response=rag_chain.invoke(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8f4136f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Answer:LangChain supports agents with conversational memory using ConversationBufferMemory and summarization memory with ConversationSummaryMemory. Agents can also interact with external APIs and databases, use tools like calculators, search APIs, or custom functions, and utilize MMR retrieval for improved diversity. Additionally, Chroma is often used for embedding-based document storage and retrieval in LangChain.\n",
      "\n",
      "Source documents are:\n",
      "\n",
      "\n",
      "Document1:\n",
      "LangChain supports conversational memory using ConversationBufferMemory and summarization memory with ConversationSummaryMemory.\n",
      "\n",
      "Document2:\n",
      "Agents in LangChain can use tools like calculators, search APIs, or custom functions based on the instructions they receive.\n",
      "\n",
      "Document3:\n",
      "MMR (Maximal Marginal Relevance) retrieval in LangChain improves diversity by balancing relevance and redundancy.\n",
      "\n",
      "Document4:\n",
      "LangChain agents can interact with external APIs and databases, enhancing the capabilities of LLM-powered applications.\n",
      "\n",
      "Document5:\n",
      "Chroma is a lightweight vector store often used in LangChain for embedding-based document storage and retrieval.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n Answer:{response['answer']}\\n\")\n",
    "print(f\"Source documents are:\\n\")\n",
    "for i,doc in enumerate(response['context']):\n",
    "    print(f\"\\nDocument{i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1251693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is Machine learning',\n",
       " 'context': [Document(id='2c5cedb6-ae8a-4a24-baa2-2482d52d571f', metadata={'source': 'mmr-rag-dataset.txt'}, page_content='LangChain is an open-source framework designed to simplify the development of applications using large language models (LLMs).'),\n",
       "  Document(id='624e1bbc-aabd-43c8-a63a-0817fa4589ee', metadata={'source': 'mmr-rag-dataset.txt'}, page_content=\"The 'map-reduce' chain breaks up large documents, processes them separately, and then aggregates the outputs.\"),\n",
       "  Document(id='81e63bb9-514a-4003-9134-c1fcb20445f0', metadata={'source': 'mmr-rag-dataset.txt'}, page_content='The framework supports integration with various vector databases like FAISS and Chroma for semantic retrieval.')],\n",
       " 'answer': \"I don't know.\"}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke({\"input\":\"what is Machine learning\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e21b05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dbcf18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGUdemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
